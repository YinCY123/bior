---
title: "SingleR"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preface

Imagine a world without a reference genome. Whenever we receive new RNA-seq data, we'd need to run it through an assembler to identify the expressed sequences. We would then need to inspect each sequence to determine its likely function, e.g., based on sequence motifs. This process is analogous to current practice in single-cell data analysis; simply replace reads with cells, assemblies with clusters, and genes with cell types. A typical practitioner will hope that their clusters are reasonable proxies for the biological states of interest and that their manual annotation of the clusters is accurate. Such an "artisanal" process is difficult to reproduce and scale to larger datasets involving more diverse cell types.

The solution is to perform automated cell type annotation, a.k.a. cell type classification (or occasionally, "label transfer"). These methods compare cells in a new dataset against curated reference profiles of known cell types, assigning each new cell to the reference type that its expression profile is most similar to. This allows users to skip the mundane annotation of their data and jump directly to the interesting questions - does my cell type change in abundance or expression across treatments? Is there interesting substructure within an existing population? In this respect, automated annotation methods are the single-cell field's equivalent to genome aligners, and we anticipate that the former will also become standard procedure for single-cell data analysis.

# Chapter 1 Introduction

## Motivation

The Bioconductor package `SingleR` implements an automatic annotation method for single-cell RNA sequencing (scRNA-seq) data (Aran et al. 2019). Given a reference dataset of samples (single-cell or bulk) with known labels, it assigns those labels to new cells from a test dataset based on similarities in their expression profiles. This provides a convenient way of transferring biological knowledge across datasets, allowing users to leverage the domain expertise implicit in the creation of each reference. The most common application of `SingleR` involves predicting cell type (or "state", or "kind") in a new dataset, a process that is facilitated by the availability of curated references and compatibility with user-supplied datasets. In this manner, the burden of manually interpreting clusters and defining marker genes only has to be done once, for the reference dataset, and this knowledge can be propagated to new datasets in an automated manner.

## Method description

`SingleR` can be considered a robust variant of `nearest-neighbors classification`, with some tweaks to improve resolution for closely related labels. For each test cell:

1.  We compute the Spearman correlation between its expression profile and that of each reference sample. The use of Spearman's correlation provides a measure of robustness to batch effects across datasets. **The calculation only uses the union of marker genes identified by pairwise between labels**.

2.  We define the per-label score as a fixed quantile (by default, 0.8) of the correlations across all samples with that label. This accounts for differences in the number of reference samples for each label, which interferes with simpler flavors of nearest neighbor classification; it also avoids penalizing classifications to heterogeneous labels by only requiring a good match to a minority of samples.

3.  We repeat the score calculation for all labels in the reference dataset. The label with the highest score is used as SingleR's prediction for this cell.

4.  We optionally perform a fine-tuning step to improve resolution between closely related labels. The reference dataset is subsetted to only include labels with scores close to the maximum; scores are recomputed using only marker genes for the subset of labels, thus focusing on the most relevant features; and this process is iterated until only one label remains.


## Quick start
We will demonstrate the use of `SingleR()` on a well-known 10X Genomics dataset (Zheng et al. 2017) with the Human Primary Cell Atlas dataset (Mabbott et al. 2013) as the reference.

```{r, message=FALSE}
# loading test data
library(TENxPBMCData)
new.data <- TENxPBMCData(dataset = "pbmc4k")

# loading reference data with ensembl annotations
library(celldex)
ref.data <- celldex::HumanPrimaryCellAtlasData(ensembl = TRUE)

# Performing predictions
library(SingleR)
predictions <- SingleR(test = new.data, 
                       assay.type.test = 1, 
                       ref = ref.data, 
                       labels = ref.data$label.main)

predictions$labels %>% table
```


# Chapter 2 Using the classic mode
## Overview
`SingleR` detects markers in a pairwise manner between labels in the reference dataset. Specifically, for each label of interest, it performs pairwise comparisons to every other label in the reference and identifies the genes that are upregulated in the label of interest for each comparison. The initial score calculation is then performed on the union of marker genes across all comparisons for all label. This approach ensures that the selected subset of features will contain genes that distinguish each label from any other label. (In contrast, other approaches that treat the "other" labels as a single group do not offer this guarantee.) It also allows the fine-tuning step to aggressively improve resolution by only using marker genes from comparisons where both labels have scores close to the maximum.

The original ("classic") marker detection algorithm used in Aran et al.(2019) identified marker genes based on their log-fold changes in each pairwise comparison. Specifically, it used the genes with the largest positive differences in the per-label median log-expression values between labels. The number of genes taken from each pairwise comparison was defined as $500\frac{2}{3}^{log_{2}(n)}$, where `n` is the number of unique labels in the reference; this scheme aimed to reduce the number of genes (and thus the computational time) as the number of labels and pairwise comparisons increased. **Classic mode is primarily intended for reference datasets that have little or no replication, a description that covers many of the bulk-derived references and precludes more complicated marker detection procedures** (Chapter 3).

## Annotating the test dataset

For demonstration purposes, we will use the Grun et al. (2016) hematopoietic stem cell (HSC) dataset from the `scRNAseq` package. The `GrunHSCData()` function conveniently returns a `SingleCellExperiment` object containing the count matrix for this dataset.

```{r}
library(scRNAseq)

sce.grun <- GrunHSCData(ensembl = TRUE)
# saveRDS(sce.grun, "/home/yincy/git/Data/Bioconductor/scRNAseq/osca/sce.grun.rds")
```

Our aim is to annotate each cell with the ImmGen reference dataset (Heng et al. 2008) from the `celldex` package. (Some further comments on the choice of reference are provided below in Section 2.5.) Calling the `ImmGenData()` function returns a `SummarizedExperiment` object containing a matrix of log-expression values with sample-level labels. We also set `ensembl=TRUE` to match the reference's gene annotation with that in the `sce` object - the default behavior is to use the gene symbol.

```{r}
library(celldex)

immgen <- celldex::ImmGenData(ensembl = TRUE)
# saveRDS(immgen, "/home/yincy/git/Data/Bioconductor/celldex/immgen.rds")
```

Each `celldex` dataset actually has three sets of labels that primarily differ in their resolution. For the purposes of this demonstration, we will use the "fine" labels in the `label.fine` metadata field, which represents the highest resolution of annotation available for this dataset.

```{r}
head(immgen$label.fine)
immgen %>% colData()
```

We perform annotation by calling `SingleR()` on our test (Grun) dataset and the reference (ImmGen) dataset, leaving the default of `de.method="classic"` to use the original marker detection scheme. This applies the algorithm described in Section 1.2, returning a `DataFrame` where each row contains prediction results for a single cell in the `sce` object. Labels are provided before fine-tuning (`first.labels`), after fine-tuning (`labels`) and after pruning (`pruned.labels`); some of the other fields are discussed in more detail in Chapter 4.

```{r}
# see 'choice of assay data' for 'assay.type.test=' explanation
pred <- SingleR(test = sce.grun, 
                ref = immgen, 
                labels = immgen$label.fine, 
                assay.type.test = 1)

colnames(pred)
```

## Interaction with quality control
Upon examining the distribution of assigned labels, we see that many of them are related to stem cells. However, there are quite a large number of more differentiated labels mixed in, which is not what we expect from a sorted population of HSCs.

```{r}
pred$labels %>% table %>% sort(decreasing = T) %>% head
```

This is probably because - despite what its name might suggest - the dataset obtained by `GrunHSCData()` actually contains more than HSCs. If we restrict our analysis to the sorted HSCs (obviously) and remove one low-quality batch (see the analysis here for the rationale) we can see that the distribution of cell type labels is more similar to what we might expect. **Low-quality cells lack information for accurate label assignment and need to be removed to enable interpretation of the results**.

```{r}
actual.hsc <- pred$labels[sce.grun$protocol == 'sorted hematopoietic stem cells' & sce.grun$sample != "JC4"]
actual.hsc %>% table %>% sort(decreasing = T) %>% head
```

Filtering the annotation results in the above manner is valid because `SingleR()` operates independently on each test cell. The annotation is orthogonal to any decisions about the relative quality of the cells in the test dataset; the same results will be obtained regardless of whether `SingleR` is run before or after quality control. This is logistically convenient as it means that the annotation does not have to be repeated if the quality control scheme (or any other downstream step, like clustering) changes throughout the lifetime of the analysis.

## Choices of assay data
**For the reference dataset, the assay matrix must contain log-transformed normalized expression values**. This is because the default marker detection scheme computes log-fold changes by subtracting the medians, which makes little sense unless the input expression values are already log-transformed. For alternative schemes, this requirement may be relaxed (e.g., Wilcoxon rank sum tests do not require transformation); similarly, if pre-defined markers are supplied, no transformation or normalization is necessary.

**For the test data, the assay data need not be log-transformed or even (scale) normalized**. This is because `SingleR()` computes Spearman correlations within each cell, which is unaffected by monotonic transformations like cell-specific scaling or log-transformation. It is perfectly satisfactory to provide the raw counts for the test dataset to `SingleR()`, which is the reason for setting `assay.type.test=1` in our previous `SingleR()` call for the Grun dataset.

**The exception to this rule occurs when comparing data from full-length technologies to the `celldex` references**. These references are intended to be comparable to data from unique molecular identifier (UMI) protocols where the expression values are less sensitive to differences in gene length. **Thus, when annotating Smart-seq2 test datasets against the `celldex` references, better performance can often be achieved by processing the test counts to transcripts-per-million values**.

We demonstrate below using another HSC dataset that was generated using the Smart-seq2 protocol (Nestorowa et al. 2016). Again, we see that most of the predicted labels are related to stem cells, which is comforting.

```{r}
sce.nest <- NestorowaHSCData()
# saveRDS(sce.nest, "/home/yincy/git/Data/Bioconductor/scRNAseq/sce.nest.rds")
# getting the exonic gene length
library(EnsDb.Mmusculus.v79)
mm.exons <- exonsBy(EnsDb.Mmusculus.v79, by = "gene")
mm.exons <- reduce(mm.exons)
mm.len <- sum(width(mm.exons))

# computing the TPMs with a simple scaling by gene length
library(scater)
keep <- intersect(names(mm.len), rownames(sce.nest))
tpm.nest <- calculateTPM(sce.nest[keep, ], lengths = mm.len[keep])

# performing the assignment
pred <- SingleR(test = tpm.nest, 
                ref = immgen, 
                labels = immgen$label.fine)

pred$labels %>% table %>% sort(decreasing = T) %>% head(10)
```

## Comments on choice of references
Unsurprisingly, the choice of reference has a major impact on the annotation results. We need to pick a reference that contains a superset of the labels that we expect to be present in our test dataset. Whether the original authors assigned appropriate labels to the reference samples is largely taken as a matter of faith; it is not entirely unexpected that some references are "better" than others depending on the quality of sample preparation. **We would also prefer a reference that is generated from a similar technology or protocol as our test dataset, though this is usually not an issue when using `SingleR()` to annotate well-defined cell types**.

Users are advised to read the relevant vignette for more details about the available references as well as some recommendations on which to use. (As an aside, the ImmGen dataset and other references were originally supplied along with `SingleR` itself but have since been migrated to the separate `celldex` package for more general use throughout Bioconductor). Of course, as we shall see in the next Chapter, it is entirely possible to supply your own reference datasets instead; all we need are log-expression values and a set of labels for the cells or samples.

# Chapter 3 Controlling marker detection
## Overview

One of the most important steps in `SingleR` (beyond the choice of reference, of course) is the derivation of the marker genes used in the score calculation. We have already introduced the classic approach in the previous chapter, but it is similarly straightforward to perform marker detection with conventional statistical tests. In particular, we identify top-ranked markers based on pairwise Wilcoxon rank sum tests or t-tests between labels; this allows us to account for the variability across cells to choose genes that are robustly upregulated in each label.

The availability of variance-aware marker detection methods is most relevant for reference datasets that contain a reasonable number (i.e., at least two) of replicate samples for each label. An obvious use case is that of single-cell datasets that are used as a reference to annotate other single-cell datasets. It is also possible for users to supply their own custom marker lists to `SingleR()`, facilitating incorporation of prior biological knowledge into the annotation process. We will demonstrate these capabilities below in this chapter.

## Annotation with test-based marker detection

To demonstrate, we will use two human pancreas scRNA-seq datasets from the `scRNAseq` package. The aim is to use one pre-labelled dataset to annotate the other unlabelled dataset. First, we set up the Muraro et al. (2016) dataset to be our reference, computing log-normalized expression values as discussed in Section 2.4.

```{r}
library(scRNAseq)

sce.murao <- MuraroPancreasData()

# removing unlabelled cells or cells without a clear label.
sce.murao$label %>% table(useNA = "always")
sce.murao <- sce.murao[, !is.na(sce.murao$label) & sce.murao$label != "unclear"]

library(scater)
sce.murao <- logNormCounts(sce.murao)
sce.murao$label %>% table
```

We then set up our test dataset from Grun et al. (2016), applying some basic quality control.

```{r}
library(dplyr)
sce.grun <- GrunPancreasData()
# saveRDS(sce.grun, "/home/yincy/git/Data/Bioconductor/scRNAseq/sce.grun.rds")

sce.grun <- addPerCellQC(sce.grun)
qc <- quickPerCellQC(colData(sce.grun), 
                           percent_subsets = "altexps_ERCC_percent", 
                           batch = sce.grun$donor, 
                           subset = sce.grun$donor %in% c("D17", "D7", "D2"))
qc
sce.grun <- sce.grun[, !qc$discard]
sce.grun <- logNormCounts(sce.grun)
```

We run `SingleR()` as described previously but with a marker detection mode that considers the variance of expression across cells. **Here, we will use the Wilcoxon ranked sum test to identify the top markers for each pairwise comparison between labels. This is slower but more appropriate for single-cell data compared to the default marker detection algorithm, as the latter may fail for low-coverage data where the median for each label is often zero**.

```{r}
pred.grun <- SingleR(test = sce.grun, 
                     ref = sce.murao, 
                     labels = sce.murao$label, 
                     de.method = "wilcox")

pred.grun$labels %>% table
```

By default, the function will take the top `de.n` (default: 10) genes from each pairwise comparison between labels. **A larger number of markers increases the robustness of the annotation by ensuring that relevant genes are not omitted, especially if the reference dataset has study-specific effects that cause uninteresting genes to dominate the top set**. However, this comes at the cost of increasing noise and computational time.

```{r}
pred.grun <- SingleR(test = sce.grun, 
                     ref = sce.murao, 
                     labels = sce.murao$label, 
                     de.method = "wilcox", 
                     de.n = 50)

pred.grun$labels %>% table
```

## Defining custom markers

The marker detection in `SingleR()` is built on top of the testing framework in `scran`, so most options in `pairwiseWilcox` and friends can be applied via the `de.args=` option. For example, we could use the t-test and test against a log-fold change threshold with `de.args=list(lfc=1)`.

```{r}
pred.grun2 <- SingleR(test = sce.grun, 
                      ref = sce.murao, 
                      labels = sce.murao$label, 
                      de.method = "t", 
                      de.args = list(lfc = 1))

pred.grun2$labels %>% table
```

However, users can also construct their own marker lists with any DE testing machinery. For example, we can perform pairwise binomial tests to identify genes that are differentially detected (i.e., have differences in the proportion of cells with non-zero counts) between labels in the reference Muraro dataset. We then take the top 10 marker genes from each pairwise comparison, obtaining a list of lists of character vectors containing the identities of the markers for that comparison.

```{r}
library(scran)
out <- pairwiseBinom(x = counts(sce.murao), 
                     groups = sce.murao$label, 
                     direction = "up")

markers <- getTopMarkers(de.lists = out$statistics, 
                         pairs = out$pairs, 
                         n = 10)
# upregulated in each acinar compared to alpha
markers$acinar$alpha
```

```{r}
# upregulated in alpha compared to acinar
markers$alpha$acinar
```

Once we have this list of lists, we supply it to `SingleR()` via the `genes=` argument, which causes the function to bypass the internal marker detection to use the supplied gene sets instead. The most obvious benefit of this approach is that the user can achieve greater control of the markers, allowing integration of prior biological knowledge to obtain more relevant genes and a more robust annotation.

```{r}
pred.grun2b <- SingleR(test = sce.grun, 
                       ref = sce.murao, 
                       labels = sce.murao$label, 
                       genes = markers)

pred.grun2b$labels %>% table
```

**In some cases, markers may only be available for specific labels rather than for pairwise comparisons between labels. This is accommodated by supplying a named list of character vectors to `genes`. Note that this is likely to be less powerful than the list-of-lists approach as information about pairwise differences is discarded**.

```{r}
# Creating label-specific markers
label.markers <- lapply(markers, unlist)
label.markers <- lapply(label.markers, unique)
label.markers %>% str
```

```{r}
pred.grun2c <- SingleR(test = sce.grun, 
                       ref = sce.murao, 
                       labels = sce.murao$label, 
                       genes = label.markers)

pred.grun2c$labels %>% table
```

## Pseudo-bulk aggregation

Single-cell reference datasets provide a like-for-like comparison to our test single-cell datasets, yielding a more accurate classification of the cells in the latter (hopefully). However, there are frequently many more samples in single-cell references compared to bulk references, increasing the computational work involved in classification. We overcome this by aggregating cells into one "pseudo-bulk" sample per label (e.g., by averaging across log-expression values) and using that as the reference profile, which allows us to achieve the same efficiency as the use of bulk references.

The obvious cost of this approach is that we discard potentially useful information about the distribution of cells within each label. Cells that belong to a heterogeneous population may not be correctly assigned if they are far from the population center. To preserve some of this information, we perform k-means clustering within each label to create pseudo-bulk samples that are representative of a particular region of the expression space (i.e., vector quantization). We create $\sqrt{N}$ clusters given a label with N cells, which provides a reasonable compromise between reducing computational work and preserving the label's internal distribution.

To enable this aggregation, we simply set `aggr.ref=TRUE` in the `SingleR()` call. This uses the `aggregateReference()` function to perform k-means clustering within each label (typically after principal components analysis on the log-expression matrix, for greater speed) and average expression values for each within-label cluster. Note that marker detection is still performed on the unaggregated data so as to make full use of the distribution of expression values across cells.

```{r}
set.seed(100)
pred.grun3 <- SingleR(test = sce.grun, 
                      ref = sce.murao, 
                      labels = sce.murao$label, 
                      de.method = "wilcox", 
                      aggr.ref = TRUE, 
                      aggr.args = list(rank = 15, power = 1/3))

pred.grun3$labels %>% table(useNA = "ifany")
```

Obviously, the aggregation itself requires computational work so setting `aggr.ref=TRUE` in `SingleR()` itself may not improve speed. Rather, the real power of this approach lies in pre-aggregating the reference dataset so that it can be repeatedly applied to quickly annotate multiple test datasets.

# Chapter 4 Annotation diagnostics

## Overview

In addition to the labels, `SingleR()` returns a number of helpful diagnostics about the annotation process that can be used to determine whether the assignments are appropriate. Unambiguous assignments corroborated by expression of canonical markers add confidence to the results; conversely, low-confidence assignments can be pruned out to avoid adding noise to downstream analyses. This chapter will demonstrate some of these common sanity checks on the pancreas datasets.

## Based on the scores within cells

The most obvious diagnostic reported by `SingleR()` is the nested matrix of per-cell scores in the scores field. This contains the correlation-based scores prior to any fine-tuning for each cell (row) and reference label (column). Ideally, we would see unambiguous assignments where, for any given cell, one label's score is clearly larger than the others.

```{r}
pred.grun$scores[1:10, ]
```

To check whether this is indeed the case, we use the `plotScoreHeatmap()` function to visualize the score matrix. Here, the key is to examine the spread of scores within each cell, i.e., down the columns of the heatmap. Similar scores for a group of labels indicates that the assignment is uncertain for those columns, though this may be acceptable if the uncertainty is distributed across closely related cell types. (**Note that the assigned label for a cell may not be the visually top-scoring label if fine-tuning is applied, as the only the pre-tuned scores are directly comparable across all labels**.)

```{r}
plotScoreHeatmap(pred.grun)
```

We can also display other metadata information for each cell by setting `clusters=` or `annotation_col=`. This is occasionally useful for examining potential batch effects, differences in cell type composition between conditions, relationship to clusters from an unsupervised analysis and so on,. Below displays the donor of origin for each cell; we can see that each cell type has contributions from multiple donors, which is reassuring as it indicates that our assignments are not (purely) driven by donor effects.

```{r}
plotScoreHeatmap(pred.grun, 
                 annotation_col = as.data.frame(colData(sce.grun)[, "donor", drop = F]))
```

**The `scores` matrix has several caveats associated with its interpretation. Only the pre-tuned scores are stored in this matrix, as scores after fine-tuning are not comparable across all labels. This means that the label with the highest score for a cell may not be the cell's final label if fine-tuning is applied**. Moreover, the magnitude of differences in the scores has no clear interpretation; indeed, `plotScoreHeatmap()` dispenses with any faithful representation of the scores and instead adjusts the values to highlight any differences between labels within each cell.

## Based on the deltas across cells

**We identify poor-quality or ambiguous assignments based on the per-cell "delta", i.e., the difference between the score for the assigned label and the median across all labels for each cell**. Our assumption is that most of the labels in the reference are not relevant to any given cell. Thus, the median across all labels can be used as a measure of the baseline correlation, while the gap from the assigned label to this baseline can be used as a measure of the assignment confidence.

Low deltas indicate that the assignment is uncertain, possibly because the cell's true label does not exist in the reference. An obvious next step is to apply a threshold on the delta to filter out these low-confidence assignments. We use the delta rather than the assignment score as the latter is more sensitive to technical effects. For example, changes in library size affect the technical noise and can increase/decrease all scores for a given cell, while the delta is somewhat more robust as it focuses on the differences between scores within each cell.

`SingleR()` will set a threshold on the delta for each label using an outlier-based strategy. Specifically, we identify cells with deltas that are small outliers relative to the deltas of other cells with the same label. This assumes that, for any given label, most cells assigned to that label are correct. We focus on outliers to avoid difficulties with setting a fixed threshold, especially given that the magnitudes of the deltas are about as uninterpretable as the scores themselves. Pruned labels are reported in the `pruned.labels` field where low-quality assignments are replaced with `NA`.

```{r}
to.remove <- is.na(pred.grun$pruned.labels)
table(Label = pred.grun$labels, Removed = to.remove)

table(labels = pred.grun$labels, pruned.labels = pred.grun$pruned.labels, useNA = "ifany")
```

However, the default pruning parameters may not be appropriate for every dataset. For example, if one label is consistently misassigned, the assumption that most cells are correctly assigned will not be appropriate. In such cases, we can revert to a fixed threshold by manually calling the underlying `pruneScores()` function with `min.diff.med=`. The example below discards cells with deltas below an arbitrary threshold of 0.2, where higher thresholds correspond to greater assignment certainty.

```{r}
to.remove <- pruneScores(pred.grun, min.diff.med = 0.2)
table(Label = pred.grun$labels, Removed = to.remove)
```

This entire process can be visualized using the `plotScoreDistribution()` function, which displays the per-label distribution of the deltas across cells. We can use this plot to check that outlier detection in `pruneScores()` behaved sensibly. Labels with especially low deltas may warrant some additional caution in their interpretation.

```{r}
plotDeltaDistribution(pred.grun, size = 1, ncol = 3)
```

**If fine-tuning was performed, we can apply an even more stringent filter based on the difference between the highest and second-highest scores after fine-tuning**. Cells will only pass the filter if they are assigned to a label that is clearly distinguishable from any other label. *In practice, this approach tends to be too conservative as assignments involving closely related labels are heavily penalized*.

```{r}
to.remove2 <- pruneScores(pred.grun, min.diff.next = 0.1)

table(Label = pred.grun$labels, Removed = to.remove2)
```

## Based on marker gene expression

**Another simple yet effective diagnostic is to examine the expression of the marker genes for each label in the test dataset. The marker genes used for each label are reported in the `metadata()` of the `SingleR()` output, so we can simply retrieve them to visualize their (usually log-transformed) expression values across the test dataset**. We use the `plotHeatmap()` function from `scater` to examine the expression of markers used to identify beta cells.

```{r}
all.markers <- metadata(pred.grun)$de.genes
beta.markers <- unique(unlist(all.markers$beta))
sce.grun$labels <- pred.grun$labels

library(scater)
plotHeatmap(sce.grun, 
            order_columns_by = "labels", 
            features = beta.markers[1:50], 
            fontsize_row =5)
```

If a cell in the test dataset is confidently assigned to a particular label, we would expect it to have strong expression of that label's markers. We would also hope that those label's markers are biologically meaningful; in this case, we do observe strong upregulation of insulin (INS) in the beta cells, which is reassuring and gives greater confidence to the correctness of the assignment. If the identified markers are not meaningful or not consistently upregulated, some skepticism towards the quality of the assignments is warranted.

In practice, the heatmap may be overwhelmingly large if there too many reference-derived markers. To resolve this, we can prune the set of markers to focus on the most interesting genes based on their test expression profiles. Below is limited to the top genes with the strongest evidence for upregulation in our test dataset using the assigned labels; such genes are effectively markers for beta cells in both the reference and test datasets. As a diagnostic plot, this is much more amenable to quick inspection to check that the expected genes are present.

```{r}
library(scran)

empirical.markers <- findMarkers(x = sce.grun, 
                                 groups = sce.grun$labels, 
                                 direction = "up")

m <- match(beta.markers, rownames(empirical.markers$beta))
m <- beta.markers[rank(m) <= 20]

library(scater)
plotHeatmap(sce.grun, 
            order_columns_by = "labels", 
            features = m)
```

It is straightforward to repeat this process for all labels by wrapping this code in a loop. Note that `plotHeatmap()` is not the only function that can be used for this visualization; we could also use `plotDots()` to create a Seurat-style dot plot, or we could use other heatmap plotting functions such as `dittoHeatmap()` from `dittoSeq`.

```{r}
collected <- list()

for(lab in unique(sce.grun$labels)){
    lab.markers <- unique(unlist(all.markers[[lab]]))
    m <- match(label.markers, rownames(empirical.markers[[lab]]))
    m <- lab.markers[rank(m) <= 20]
    collected[[lab]] <- plotHeatmap(sce.grun, silent = T, 
                                    order_columns_by = "labels", 
                                    main = lab, 
                                    features = m, 
                                    fontsize_row = 3)[[4]]
}

cowplot::plot_grid(plotlist = collected, 
                   ncol = 2)
```

In general, the heatmap provides a more interpretable diagnostic visualization than the plots of scores and deltas. However, it does require more effort to inspect and may not be feasible for large numbers of labels. It is also difficult to use a heatmap to determine the correctness of assignment for closely related labels.

## Comparing to unsupervised clustering

**It can also be instructive to compare the assigned labels to the groupings generated from unsupervised clustering algorithms. The assumption is that the differences between reference labels are also the dominant factor of variation in the test dataset; this implies that we should expect strong agreement between the clusters and the assigned labels**. To demonstrate, we'll use the sceG from Chapter 8 where clusters have generated using a graph-based method (Xu and Su 2015).

```{r}
#--- loading-muraro ---#
library(scRNAseq)
sceM <- MuraroPancreasData()
sceM <- sceM[,!is.na(sceM$label) & sceM$label!="unclear"] 

#--- normalize-muraro ---#
library(scater)
sceM <- logNormCounts(sceM)

#--- loading-grun ---#
sceG <- GrunPancreasData()

sceG <- addPerCellQC(sceG)
qc <- quickPerCellQC(colData(sceG), 
    percent_subsets="altexps_ERCC_percent",
    batch=sceG$donor,
    subset=sceG$donor %in% c("D17", "D7", "D2"))
sceG <- sceG[,!qc$discard]

#--- normalize-grun ---#
sceG <- logNormCounts(sceG)

#--- annotation ---#
library(SingleR)
pred.grun <- SingleR(test=sceG, ref=sceM, labels=sceM$label, de.method="wilcox")

#--- clustering ---#
library(scran)
decG <- modelGeneVarWithSpikes(sceG, "ERCC")

set.seed(1000100)
sceG <- denoisePCA(sceG, decG)

library(bluster)
sceG$cluster <- clusterRows(reducedDim(sceG), NNGraphParam(k=5))
```

We compare these clusters to the labels generated by `SingleR`. Any similarity can be quantified with the adjusted rand index (ARI) with `pairwiseRand()` from the `bluster` package. Large ARIs indicate that the two partitionings are in agreement, though an acceptable definition of "large" is difficult to gauge; experience suggests that a reasonable level of consistency is achieved at ARIs above 0.5.

```{r}
library(bluster)

pairwiseRand(ref = sceG$cluster, alt = pred.grun$labels, mode = "index")
```

In practice, it is more informative to examine the distribution of cells across each cluster/label combination. Below shows that most clusters are nested within labels, a difference in resolution that is likely responsible for reducing the ARI. Clusters containing multiple labels are particularly interesting for diagnostic purposes, as this suggests that the differences between labels are not strong enough to drive formation of distinct clusters in the test.

```{r}
tab <- table(cluster = sceG$cluster, label = pred.grun$labels)
pheatmap::pheatmap(log10(tab + 10))
```

The underlying assumption is somewhat reasonable in most scenarios where the labels relate to cell type identity. However, disagreements between the clusters and labels should not be cause for much concern. The whole point of unsupervised clustering is to identify novel variation that, by definition, is not in the reference. It is entirely possible for the clustering and labels to be different without compromising the validity or utility of either; the former captures new heterogeneity while the latter facilitates interpretation in the context of existing knowledge.

# Chapter 5 Using multiple references

## Overview

In some cases, we may wish to use multiple references for annotation of a test dataset. This yields a more comprehensive set of cell types that are not covered by any individual reference, especially when differences in the resolution are considered. However, it is not trivial due to the presence of batch effects across references (from differences in technology, experimental protocol or the biological system) as well as differences in the annotation vocabulary between investigators.

Several strategies are available to combine inferences from multiple references:

- using reference-specific labels in a combined reference  
- using harmonized labels in a combined reference  
- combining scores across multiple reference  

This chapter discusses the various strengths and weaknesses of each strategy and provides some practical demonstrations of each. Here, we will use the HPCA and Blueprint/ENCODE datasets as our references and (yet another) PBMC dataset as the test.

```{r}
library(TENxPBMCData)
pbmc <- TENxPBMCData("pbmc8k")
# saveRDS(pbmc, file = "/home/yincy/git/Data/Bioconductor/TENxPBMCData/pbmc8k.rds")

library(celldex)
hpca <- HumanPrimaryCellAtlasData(ensembl = TRUE)
bpe <- BlueprintEncodeData(ensembl = TRUE)

# saveRDS(hpca, "/home/yincy/git/Data/Bioconductor/celldex/hpca.rds")
# saveRDS(bpe, "/home/yincy/git/Data/Bioconductor/celldex/bpe.rds")
```

## Using reference-specific labels

In this strategy, each label is defined in the context of its reference dataset. This means that a label - say, "B cell" - in reference dataset X is considered to be different from a "B cell" label in reference dataset Y. Use of reference-specific labels is most appropriate if there are relevant biological differences between the references; for example, if one reference is concerned with healthy tissue while the other reference considers diseased tissue, it can be helpful to distinguish between the same cell type in different biological contexts.

We can easily implement this approach by combining the expression matrices together and pasting the reference name onto the corresponding character vector of labels. This modification ensures that the downstream `SingleR()` call will treat each label-reference combination as a distinct entity.

```{r}
hpca2 <- hpca
hpca2$label.main <- paste("HPCA.", hpca2$label.main, sep = "")

bpe2 <- bpe
bpe2$label.main <- paste("BPE.", bpe2$label.main, sep = "")

shared <- intersect(rownames(hpca2), rownames(bpe2))
combined <- cbind(hpca2[shared, ], bpe2[shared, ])
```

It is then straightforward to perform annotation with the usual methods.

```{r}
library(SingleR)

com.res1 <- SingleR(test = pbmc, 
                    ref = combined, 
                    labels = combined$label.main, 
                    assay.type.test = "counts")

com.res1$labels %>% table
```

However, this strategy identifies markers by directly comparing expression values across references, meaning that the marker set is likely to contain genes responsible for uninteresting batch effects. This will increase noise during the calculation of the score in each reference, possibly leading to a loss of precision and a greater risk of technical variation dominating the classification results. The use of reference-specific labels also complicates interpretation of the results as the cell type is always qualified by its reference of origin.

## Comparing scores across references

### Combining inferences from individual references

Another strategy - and the default approach implemented in `SingleR()` - involves performing classification separately within each reference, and then collating the results to choose the label with the highest score across references. This is a relatively expedient approach that avoids the need for explicit harmonization while also reducing exposure to reference-specific batch effects.

To use this method, we simply pass multiple objects to the `ref=` and `label=` argument in `SingleR()`. The combining strategy is as follows:

1.  The function first annotates the test dataset with each reference individually in the same manner as described in Section 1.2. This step is almost equivalent to simply looping over all individual references and running `SingleR()` on each.

2.  For each cell, the function collects its predicted labels across all references. In doing so, it also identifies the union of markers that are upregulated in the predicted label in each reference.

3.  The function identifies the overall best-scoring label as the final prediction for that cell. This step involves a recomputation of the scores across the identified marker subset to ensure that these scores are derived from the same set of genes (and are thus comparable across references).

The function will then return a `DataFrame` of combined results for each cell in the test dataset, including the overall label and the reference from which it was assigned.

```{r}
com.res2 <- SingleR(test = pbmc, 
                    assay.type.test = 1, 
                    ref = list(BPE = bpe, HPCA = hpca), 
                    labels = list(bpe$label.main, hpca$label.main))

com.res2$labels %>% table
```

```{r}
com.res2$reference %>% table
```

The main appeal of this approach lies in the fact that it is based on the results of annotation with individual references. This avoids batch effects from comparing expression values across references; it reduces the need for any coordination in the label scheme between references; and simultaneously provides the per-reference annotations in the results. The last feature is particularly useful as it allows for more detailed diagnostics, troubleshooting and further analysis.

```{r}
com.res2$orig.results$BPE$labels %>% head
```

```{r}
com.res2$orig.results$HPCA$labels %>% head
```

The main downside is that it is somewhat suboptimal if there are many labels that are unique to one reference, as markers are not identified with the aim of distinguishing a label in one reference from another label in another reference. The continued lack of consistency in the labels across references also complicates interpretation of the results, though we can overcome this by using harmonized labels as described below.

### Combined diagnostics

All of the diagnostic plots in `SingleR` will naturally operate on these combined results. For example, we can create a heatmap of the scores in all of the individual references as well as for the recomputed scores in the combined results. Note that scores are only recomputed for the labels predicted in the individual references, so all labels outside of those are simply set to `NA` - hence the swathes of grey.

```{r}
plotScoreHeatmap(com.res2)
```

The deltas for each individual reference can also be plotted with `plotDeltaDistribution()`. No deltas are shown for the recomputed scores as the assumption described in Section 4.3 may not be applicable across the predicted labels from the individual references. For example, if all individual references suggest the same cell type with similar recomputed scores, any delta would be low even though the assignment is highly confident.

```{r}
plotDeltaDistribution(com.res2, ncol = 3)
```

We can similarly extract marker genes to use in heatmaps as described in Section 4.4. As annotation was performed to each individual reference, we can simply extract the marker genes from the nested DataFrames as shown in Figure 5.3.

```{r}
hpca.markers <- metadata(com.res2$orig.results$HPCA)$de.genes
bpe.markers <- metadata(com.res2$orig.results$BPE)$de.genes
mono.markers <- unique(unlist(hpca.markers$Monocyte, bpe.markers$Monocytes))

library(scater)

plotHeatmap(logNormCounts(pbmc), 
            order_columns_by = list(I(com.res2$labels)), 
            features = mono.markers)
```

## Using harmonized labels

### Sharing information during marker detection

One of the major problems with using multiple references is the presence of study-specific nomenclature. For example, the concept of a `B cell` may be annotated as `B cells` in one reference, B_cells in another reference, and then `B` and `B-cell` and so on in other references. We can overcome this by using harmonized labels where the same cell type is assigned as the same label across references, simplifying interpretation and ensuring that irrelevant discrepancies in labelling do not intefere with downstream analysis.

Many of the `SingleR` reference datasets already have their labels mapped to the [Cell Ontology](https://www.ebi.ac.uk/ols/ontologies/cl), which provides a standard vocabulary to refer to the same cell type across diverse datasets. We will describe the utility of Cell Ontology terms in more detail in Chapter 6; at this point, the key idea is that the same term is used for the same conceptual cell type in each reference. To simplify interpretation, we set `cell.ont="nonna"` to remove all samples that could not be mapped to the ontology.  

```{r}
hpca.ont <- HumanPrimaryCellAtlasData(ensembl = TRUE, cell.ont = "nonna")
bpe.ont <- BlueprintEncodeData(ensembl = TRUE, cell.ont = "nonna")

# using the same set of genes
shared <- intersect(rownames(hpca.ont), rownames(bpe.ont))
hpca.ont <- hpca.ont[shared, ]
bpe.ont <- bpe.ont[shared, ]

# showing the top 10 most frequent terms
hpca.ont$label.ont %>% table %>% sort(decreasing = T) %>% head(10)
```

```{r}
bpe.ont$label.ont %>% table %>% sort(decreasing = T) %>% head(10)
```


The simplest way to take advantage of the standardization in terminology is to use label.ont in place of `label.main` in the previous section's `SingleR()` call. This yields annotations that have follow the same vocabulary regardless of the reference used for assignment.

The simplest way to take advantage of the standardization in terminology is to use `label.ont` in place of `label.main` in the previous section’s `SingleR()` call. This yields annotations that have follow the same vocabulary regardless of the reference used for assignment.  

```{r}
com.res3a <- SingleR(test = pbmc, 
                     assay.type.test = 1, 
                     ref = list(BPE = bpe.ont, HPCA = hpca.ont), 
                     labels = list(bpe.ont$label.ont, hpca.ont$label.ont))

table(Label = com.res3a$labels, Reference = com.res3a$reference)
```


A more advanced approach is to share information across references during the marker detection stage. This is done by favoring genes the exhibit upregulation consistently in multiple references, which increases the likelihood that those markers will generalize to other datasets. For classic marker detection, we achieve this by calling `getClassicMarkers()` to obtain markers for use in `SingleR()`; the same effect can be achieved for test-based methods in `scran` functions by setting `block=`. We then use these improved markers by passing them to `genes=` as described in Section 3.3. In this case, we specify `com.markers` twice in a list to indicate that we are using them for both of our references.

A more advanced approach is to share information across references during the marker detection stage. This is done by favoring genes the exhibit upregulation consistently in multiple references, which increases the likelihood that those markers will generalize to other datasets. For classic marker detection, we achieve this by calling `getClassicMarkers()` to obtain markers for use in `SingleR()`; the same effect can be achieved for test-based methods in `scran` functions by setting `block=`. We then use these improved markers by passing them to `genes=` as described in Section 3.3. In this case, we specify `com.markers` twice in a list to indicate that we are using them for both of our references.   

```{r}
com.markers <- getClassicMarkers(ref = list(BPE = bpe.ont, HPCA = hpca.ont), 
                                 labels = list(bpe.ont$label.ont, hpca.ont$label.ont))

com.res3b <- SingleR(test = pbmc, 
                     assay.type.test = 1, 
                     ref = list(BPE = bpe.ont, HPCA = hpca.ont), 
                     labels = list(bpe.ont$label.ont, hpca.ont$label.ont), 
                     genes = list(com.markers, com.markers))

table(Label = com.res3b$labels, Reference = com.res3b$reference)
```

It is worth noting that, in the above code, the DE genes are still identified within each reference and then the statistics are merged across references to identify the top markers. This ensures that we do not directly compare expression values across references, which reduces the susceptibility of marker detection to batch effects.  

The most obvious problem with this approach is that it assumes that harmonized labels are available. This is usually not true and requires some manual mapping of the author-provided labels to a common vocabulary. The mapping process also runs the risk of discarding relevant information about the biological status (e.g., activation status, disease condition) if there is no obvious counterpart for that state in the ontology.  

### Manual label harmonization  
The `matchReferences()` function provides a simple approach for label harmonization between two references. Each reference is used to annotate the other and the probability of mutual assignment between each pair of labels is computed, i.e., for each pair of labels, what is the probability that a cell with one label is assigned the other and vice versa? Probabilities close to 1 indicate there is a 1:1 relation between that pair of labels; on the other hand, an all-zero probability vector indicates that a label is unique to a particular reference.  

```{r}
bp.se <- BlueprintEncodeData()
hpca.se <- HumanPrimaryCellAtlasData()

matched <- matchReferences(ref1 = bp.se, 
                           ref2 = hpca.se, 
                           labels1 = bp.se$label.main, 
                           labels2 = hpca.se$label.main)

pheatmap::pheatmap(matched, color = viridis(100, option = "B"))
```

This function can be used to guide harmonization to enforce a consistent vocabulary between two sets of labels. However, some manual intervention is still required in this process given the ambiguities posed by differences in biological systems and technologies. In the example above, neurons are considered to be unique to each reference while smooth muscle cells in the HPCA data are incorrectly matched to fibroblasts in the Blueprint/ENCODE data. CD4+ and CD8+ T cells are also both assigned to “T cells”, so some decision about the acceptable resolution of the harmonized labels is required here.  

As an aside, we can also use this function to identify the matching clusters between two independent scRNA-seq analyses. This involves substituting the cluster assignments as proxies for the labels, allowing us to match up clusters and integrate conclusions from multiple datasets without the difficulties of batch correction and reclustering.  


# Chapter 6 Exploting the cell ontology
## Motivation
As previously discussed in Section 5.4, SingleR maps the labels in its references to the Cell Ontology. The most obvious advantage of doing this is to provide a standardized vocabulary with which to describe cell types, thus facilitating integrated analyses with multiple references. However, another useful feature of the Cell Ontology is its hierarchical organization of terms, allowing us to adjust cell type annotations to the desired resolution. This represents a more dynamic alternative to the static label.main and label.fine options in each reference.  

## Basic manipulation
We use the `ontoProc` package to load in the Cell Ontology. This produces an `ontology_index` object (from the `ontologyIndex` package) that we can query for various pieces of information.  

```{r}
library(ontoProc)

cl <- getCellOnto()
```

The most immediate use of this object lies in mapping ontology terms to their plain-English descriptions. We can use this to translate annotations produced by `SingleR()` from the label.ont labels into a more interpretable form. We demonstrate this approach using `celldex`’s collection of mouse RNA-seq references (Aran et al. 2019).  

```{r}
cl$name %>% head
```

```{r}
cl$def %>% head
```

```{r}
library(celldex)

ref <- MouseRNAseqData(cell.ont = "nonna")
translated <- cl$name[ref$label.ont]
translated %>% head
```

Another interesting application involves examining the relationship between different terms. The ontology itself is a directed acyclic graph, so we can can convert it into `graph` object for advanced queries using the `igraph` package. Each edge represents an “is a” relationship where each vertex represents a specialized case of the concept of the parent node.  

```{r}
parents <- cl$parents
self <- rep(names(parents), length(parents))

library(igraph)
g <- make_graph(rbind(unlist(parents), self))
```

One query involves identifying all descendents of a particular term of interest. This can be useful when searching for a cell type in the presence of variable annotation resolution; for example, a search for “epithelial cell” can be configured to pick up all child terms such as "endothelial cell" and "ependymal cell".  

```{r}
term <- "CL:0000624"
cl$name[term]
```

```{r}
all.kids <- names(subcomponent(graph = g, v = term, mode = "out"))
cl$name[all.kids] %>% head
```

Alternatively, we might be interested in the last common ancestor (LCA) for a set of terms. This is the furthest term - or, in some cases, multiple terms - from the root of the ontology that is also an ancestor of all of the terms of interest. We will use this LCA concept in the next section to adjust resolution across multiple references.  

```{r}
terms <- c("CL:0000624", "CL:0000785", "CL:0000623")
cl$name[terms]
```

```{r}
all.ancestors <- lapply(terms, subcomponent, graph = g, mode = "in")
all.ancestors <- lapply(all.ancestors, names)
common.ancestors <- Reduce(intersect, all.ancestors)

ancestors.of.ancestors <- lapply(common.ancestors, subcomponent, graph = g, mode = "in")
ancestors.of.ancestors <- lapply(ancestors.of.ancestors, names)
ancestors.of.ancestors <- mapply(setdiff, ancestors.of.ancestors, common.ancestors)

latest.common.ancestors <- setdiff(common.ancestors, unlist(ancestors.of.ancestors))
cl$name[latest.common.ancestors]
```


## Adjusting resolution
We can use the ontology graph to adjust the resolution of the reference labels by rolling up overly-specific terms to their LCA. The `findCommonAncestors()` utility takes a set of terms and returns a list of potential LCAs for various subsets of those terms. Users can inspect this list to identify LCAs at the desired resolution and then map their descendent terms to those LCAs.  

```{r}
lca <- findCommonAncestors(ref$label.ont, g = g, names = cl$name)
lca %>% head
```

We can also use this function to synchronize multiple sets of terms to the same resolution. Here, we consider the ImmGen dataset (Heng et al. 2008), which provides highly resolved annotation of immune cell types. The `findCommonAncestors()` function specifies the origins of the descendents for each LCA, allowing us to focus on LCAs that have representatives in both sets of terms.  

```{r}
ref2 <- ImmGenData(cell.ont = "nonna")
lca2 <- findCommonAncestors(MouseRNA = ref$label.ont, 
                            ImmGen = ref2$label.ont, 
                            g = g, 
                            names = cl$name)
```

For example, we might notice that the mouse RNA-seq reference only has a single “T cell” term. To synchronize resolution across references, we would need to roll up all of the ImmGen’s finely resolved subsets into that LCA as shown below. Of course, this results in some loss of precision and information; whether this is an acceptable price for simpler interpretation is a decision that is left to the user.  

```{r}
children <- lca2$`CL:0000084`$descendents
```

```{r}
# Synchronization:
synced.mm <- ref$label.ont
synced.mm[synced.mm %in% rownames(children)] <- "CL:0000084"
synced.ig <- ref2$label.ont
synced.ig[synced.ig %in% rownames(children)] <- "CL:0000084"
```


# Chapter 7 Advanced options  
Advanced users can split the `SingleR()` workflow into two separate training and classification steps. This means that training (e.g., marker detection, assembling of nearest-neighbor indices) only needs to be performed once for any reference. The resulting data structure can then be re-used across multiple classifications with different test datasets, provided the gene annotation in the test dataset is identical to or a superset of the genes in the training set. To illustrate, we will consider the DICE reference dataset (Schmiedel et al. 2018) from the `celldex` package.  

```{r}
library(celldex)

dice <- DatabaseImmuneCellExpressionData(ensembl = TRUE)
```


```{r}
table(dice$label.fine)
```

Let's say we want ot use the DICE reference to annotate the PBMC dataset.  

```{r}
library(TENxPBMCData)
sce.pbmc <- TENxPBMCData("pbmc3k")
```

We use the `trainSingleR()` function to do all the necessary calculations that are independent of the test dataset. (Almost; see comments below about `common`.) This yields a list of various components that contains all identified marker genes and precomputed rank indices to be used in the score calculation. We can also turn on aggregation with `aggr.ref=TRUE` (Section @ref(pseudo-bulk aggregation)) to further reduce computational work.  

```{r}
common <- intersect(rownames(sce.pbmc), rownames(dice))

library(SingleR)
set.seed(2000)
trained <- trainSingleR(dice[common, ], 
                        labels = dice$label.fine, 
                        aggr.ref = TRUE)
```

We then use the `trained` object to annotate our dataset of interest through the `classifySingleR()` function. As we can see, this yields exactly the same result as applying `SingleR()` directly. The advantage here is that trained can be re-used for multiple `classifySingleR()` calls - possibly on different datasets - without having to repeat unnecessary steps when the reference is unchanged.  

```{r}
pred <- classifySingleR(sce.pbmc, trained, assay.type = 1)

pred$labels %>% table
```

The big caveat is that the universe of genes in the test dataset must be a superset of that the reference. This is the reason behind the intersection to `common` genes and the subsequent subsetting of dice. Practical use of preconstructed indices is best combined with some prior information about the gene-level annotation; for example, we might know that we always use a particular version of the Ensembl gene models, so we would filter out any genes in the reference dataset that are not in our test datasets.  

## Parallelization
Parallelization is an obvious approach to increasing annotation throughput. This is done using the framework in the `BiocParallel` package, which provides several options for parallelization depending on the available hardware. On POSIX-compliant systems (i.e., Linux and MacOS), the simplest method is to use forking by passing `MulticoreParam()` to the `BPPARAM=` argument:  

```{r}
library(BiocParallel)

pred2a <- SingleR(test = sce.pbmc, 
                  ref = dice, 
                  assay.type.test = 1, 
                  labels = dice$label.fine, 
                  BPPARAM = MulticoreParam(2)) # 2 CPUs
```


Alternatively, one can use separate processes with `SnowParam()`, which is slower but can be used on all systems - including Windows, our old nemesis.  

```{r}
pred2b <- SingleR(test = sce.pbmc, 
                  ref = dice, 
                  assay.type.test = 1, 
                  labels = dice$label.file, 
                  BPPARAM = SnowParam(2))
```

When working on a cluster, passing `BatchtoolsParam()` to `SingleR()` allows us to seamlessly interface with various job schedulers like SLURM, LSF and so on. This permits heavy-duty parallelization across hundreds of CPUs for highly intensive jobs.  


## Approximate algorithms
It is possible to sacrifice accuracy to squeeze more speed out of `SingleR`. The most obvious approach is to simply turn off the fine-tuning with `fine.tune=FALSE`, which avoids the time-consuming fine-tuning iterations. **When the reference labels are well-separated, this is probably an acceptable trade-off**.  

```{r}
pred3a <- SingleR(test = sce.pbmc, 
                  ref = dice, 
                  assay.type.test = 1, 
                  labels = dice$label.main, 
                  fine.tune = F)
```

Another approximation is based on the fact that the initial score calculation is done using a nearest-neighbors search. By default, this is an exact seach but we can switch to an approximate algorithm via the `BNPARAM=` argument. In the example below, we use the Annoy algorithm via the BiocNeighbors framework, which yields mostly similar results. (Note, though, that the Annoy method does involve a considerable amount of overhead, so for small jobs it will actually be slower than the exact search.)  

```{r}
library(BiocNeighbors)

pred3b <- SingleR(test = sce.pbmc, 
                  ref = dice, 
                  assay.type.test = 1, 
                  labels = dice$label.main, 
                  fine.tune = F, 
                  BNPARAM = AnnoyParam())
```

## Cluster-level annotation
The default philosophy of SingleR is to perform annotation of each individual cell in the test dataset. An alternative strategy is to perform annotation of aggregated profiles for groups or clusters of cells. To demonstrate, we will perform a quick-and-dirty clustering of our PBMC dataset with a variety of Bioconductor packages.  

```{r}
library(scuttle)
sce.pbmc <- logNormCounts(sce.pbmc)

library(scran)
dec <- modelGeneVarByPoisson(sce.pbmc)
sce.pbmc <- denoisePCA(sce.pbmc, dec, subset.row = getTopHVGs(dec, n = 5000))

library(bluster)
colLabels(sce.pbmc) <- clusterRows(reducedDim(sce.pbmc), NNGraphParam())

library(scater)
set.seed(117)
sce.pbmc <- runTSNE(sce.pbmc, dimred = "PCA")
plotTSNE(sce.pbmc, colour_by = "label")
```

By passing `clusters=` to `SingleR()`, we direct the function to compute an aggregated profile per cluster. Annotation is then performed on the cluster-level profiles rather than on the single-cell level. This has the major advantage of being much faster to compute as there are obviously fewer clusters than cells; it is also easier to interpret as it directly returns the likely cell type identity of each cluster.  

```{r}
SingleR(test = sce.pbmc, 
        ref = dice, 
        clusters = colLabels(sce.pbmc), 
        labels = dice$label.main)
```

This approach assumes that each cluster in the test dataset corresponds to exactly one reference label. If a cluster actually contains a mixture of multiple labels, this will not be reflected in its lone assigned label. (We note that it would be very difficult to determine the composition of the mixture from the `SingleR()` scores.) Indeed, there is no guarantee that the clustering is driven by the same factors that distinguish the reference labels, decreasing the reliability of the annotations when novel heterogeneity is present in the test dataset. The default per-cell strategy is safer and provides more information about the ambiguity of the annotations, which is important for closely related labels where a close correspondence between clusters and labels cannot be expected.  



